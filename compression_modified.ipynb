{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compression_modified.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk1BrmT822B+yhZjeqtAfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharris50/Test/blob/master/compression_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnav5VIGqFl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "cellView": "both",
        "outputId": "94e94c9d-6173-4f98-fd1b-b5d04954fa6b"
      },
      "source": [
        "#@title\n",
        "#%%\n",
        "\n",
        "\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "#%%\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "import math\n",
        "\n",
        "\n",
        "#used to fix bug in keras preprocessing scope\n",
        "temp = tf.zeros([4, 32, 32, 3])  # Or tf.zeros\n",
        "preprocess_input(temp)\n",
        "print(\"processed\")\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# IMAGE_SIZE = (224, 224)\n",
        "# TRAIN_SIZE = 50000\n",
        "# VALIDATION_SIZE = 10000\n",
        "# BATCH_SIZE_PER_GPU = 64\n",
        "# global_batch_size = (BATCH_SIZE_PER_GPU * 1)\n",
        "# NUM_CLASSES = 10\n",
        "# TEST = 1\n",
        "# EPOCHS = 20 if TEST == 1 else 2\n",
        "\n",
        "# for command line arguments - sys.argv[1] contains the config file\n",
        "import sys\n",
        "\n",
        "# for reading the config file\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read('variables.ini')\n",
        "\n",
        "# set variables\n",
        "IMAGE_SIZE = (int(config['Image Size']['width']), int(config['Image Size']['length']))\n",
        "TRAIN_SIZE = int(config['Other']['train size'])\n",
        "VALIDATION_SIZE = int(config['Other']['validation size'])\n",
        "BATCH_SIZE_PER_GPU = int(config['Other']['batch size per gpu'])\n",
        "global_batch_size = (BATCH_SIZE_PER_GPU * 1)\n",
        "NUM_CLASSES = int(config['Other']['number of classes'])\n",
        "EPOCHS = int(config['Other']['epochs'])\n",
        "NUM_PROC = int(config['Other']['num_proc'])\n",
        "\n",
        "# import functions from util file\n",
        "from utils_modified.py import *\n",
        "\n",
        "#%% \n",
        "# Dataset code\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "dataset, info = tfds.load('cifar10', with_info=True)\n",
        "\n",
        "\n",
        "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train.shuffle(buffer_size=1000).batch(global_batch_size).repeat()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "test_dataset = dataset['test'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(global_batch_size).repeat()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('./base_model_cifar10_vgg16.h5')\n",
        "model.compile(optimizer=tf.optimizers.SGD(learning_rate=.01, momentum=.9, nesterov=True), loss='mse', metrics=['acc'])\n",
        "OG = model.evaluate(test_dataset, steps=VALIDATION_SIZE//global_batch_size//TEST)\n",
        "print(OG)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "import pprint\n",
        "targets = []\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if layer.__class__.__name__ == \"Conv2D\":\n",
        "        if layer.kernel_size[0] == 3:\n",
        "            #print(f'{i} layer {layer.name} , kernel size {layer.kernel_size}')\n",
        "            targets.append({'name': layer.name, 'layer': i})\n",
        "\n",
        "pprint.pprint(targets)\n",
        "\n",
        "\n",
        "\n",
        "for target in targets:\n",
        "\n",
        "    writer = tf.summary.create_file_writer(f\"./summarys/vgg/cifar10/{target['name']}\")\n",
        "    with writer.as_default():\n",
        "        print(f\"training layer {target['name']}\")\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = tf.keras.models.load_model('base_model_cifar10_vgg16.h5')\n",
        "        in_layer = target['layer']\n",
        "        get_output = tf.keras.Model(inputs=model.input, outputs=[model.layers[in_layer - 1].output,\n",
        "                                                                model.layers[in_layer].output])\n",
        "\n",
        "\n",
        "        replacement_layers = build_replacement(get_output, layers=2)\n",
        "        replacement_len = len(replacement_layers.layers)\n",
        "        layer_train_gen = LayerBatch(get_output, train_dataset)\n",
        "        layer_test_gen = LayerTest(get_output, test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        MSE = tf.losses.MeanSquaredError()\n",
        "\n",
        "        optimizer=tf.keras.optimizers.SGD(.00001, momentum=.9, nesterov=True)\n",
        "        replacement_layers.compile(loss=MSE, optimizer=optimizer)\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=5, min_lr=.0001, factor=.3, verbose=1)\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(patience=15, min_delta=.0001, restore_best_weights=True, verbose=1)\n",
        "\n",
        "        replacement_layers.save('/tmp/layer.h5')\n",
        "\n",
        "        for epoch in range(EPOCHS + 1):\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = tf.keras.models.load_model('base_model_cifar10_vgg16.h5')\n",
        "            in_layer = target['layer']\n",
        "            get_output = tf.keras.Model(inputs=model.input, outputs=[model.layers[in_layer - 1].output,\n",
        "                                                                    model.layers[in_layer].output])\n",
        "\n",
        "\n",
        "\n",
        "            layer_train_gen = LayerBatch(get_output, train_dataset)\n",
        "            layer_test_gen = LayerTest(get_output, test_dataset)\n",
        "\n",
        "            replacement_layers = tf.keras.models.load_model('/tmp/layer.h5')\n",
        "\n",
        "            history = replacement_layers.fit(x=layer_train_gen,\n",
        "                                        epochs=1,\n",
        "                                        steps_per_epoch=TRAIN_SIZE // global_batch_size // TEST,\n",
        "                                        validation_data=layer_test_gen,\n",
        "                                        shuffle=False,\n",
        "                                        callbacks=[reduce_lr, early_stop],\n",
        "                                        validation_steps=VALIDATION_SIZE // global_batch_size // TEST,\n",
        "                                        verbose=1)\n",
        "\n",
        "            replacement_layers.save('/tmp/layer.h5')\n",
        "\n",
        "            target['weights'] = [replacement_layers.layers[1].get_weights(), replacement_layers.layers[3].get_weights()]\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "            model = tf.keras.models.load_model('base_model_cifar10_vgg16.h5')\n",
        "            layer_name = target['name']\n",
        "            layer_pos = target['layer']\n",
        "            filters = model.layers[layer_pos].output.shape[-1]\n",
        "\n",
        "            new_model = replace_layer(model, layer_name, lambda x: replac(x, filters))\n",
        "            new_model.layers[layer_pos].set_weights(target['weights'][0])\n",
        "            new_model.layers[layer_pos + 2].set_weights(target['weights'][1])\n",
        "            new_model.compile(optimizer=tf.keras.optimizers.SGD(.1), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "            target['score'] = new_model.evaluate(test_dataset, steps=VALIDATION_SIZE // global_batch_size // TEST)\n",
        "\n",
        "            tf.summary.scalar(name='rep_loss', data=history.history['loss'][0], step=epoch)\n",
        "            tf.summary.scalar(name='val_loss', data=history.history['val_loss'][0], step=epoch)\n",
        "            tf.summary.scalar(name='model_acc', data=target['score'][1], step=epoch)\n",
        "            tf.summary.scalar(name='model_loss', data=target['score'][0], step=epoch)\n",
        "\n",
        "            writer.flush()\n",
        "            print(f\"epoch: {epoch}, rep loss {history.history['loss']}, val loss {history.history['val_loss']}, model acc {target['score'][1]}\")\n",
        "\n",
        "            # if np.abs(OG[1] - target['score'][1]) < 0.0001:\n",
        "            #     print('stoping early')\n",
        "            #     break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.models.load_model('./base_model_cifar10_vgg16.h5')\n",
        "\n",
        "\n",
        "writer = tf.summary.create_file_writer(f\"./summarys/vgg/cifar10/final_model\")\n",
        "with writer.as_default():\n",
        "    for target in targets[::-1]:\n",
        "        print(f'replacing layer {target[\"name\"]}')\n",
        "\n",
        "        layer_name = target['name']\n",
        "        layer_pos = target['layer']\n",
        "        filters = model.layers[layer_pos].output.shape[-1]\n",
        "\n",
        "\n",
        "\n",
        "        new_model = replace_layer(model, layer_name, lambda x: replac(x, filters))\n",
        "        new_model.layers[layer_pos].set_weights(target['weights'][0])\n",
        "        new_model.layers[layer_pos + 2].set_weights(target['weights'][1])\n",
        "\n",
        "        new_model.save('cifar10_vgg_modified.h5')\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = tf.keras.models.load_model('cifar10_vgg_modified.h5')\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = tf.keras.models.load_model('cifar10_vgg_modified.h5')\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(.1), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "    final = model.evaluate(test_dataset, steps=VALIDATION_SIZE // global_batch_size // TEST)\n",
        "\n",
        "    tf.summary.scalar(name='model_acc', data=final[1], step=0)\n",
        "    tf.summary.scalar(name='model_loss', data=final[0], step=0)\n",
        "\n",
        "    writer.flush()\n",
        "\n",
        "\n",
        "#%%\n",
        "# for target in targets:\n",
        "\n",
        "#     print(f'Replacing Layer {target[\"name\"]}')\n",
        "\n",
        "#     tf.keras.backend.clear_session()\n",
        "\n",
        "#     model = tf.keras.models.load_model('base_model_cifar10_vgg16.h5')\n",
        "\n",
        "#     layer_name = target['name']\n",
        "#     layer_pos = target['layer']\n",
        "#     filters = model.layers[layer_pos].output.shape[-1]\n",
        "\n",
        "\n",
        "#     new_model = replace_layer(model, layer_name, lambda x: replac(x, filters))\n",
        "#     new_model.layers[layer_pos].set_weights(target['weights'][0])\n",
        "#     new_model.layers[layer_pos + 2].set_weights(target['weights'][1])\n",
        "#     new_model.compile(optimizer=tf.keras.optimizers.SGD(.1), loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "#     target['score'] = new_model.evaluate(test_dataset, steps=VALIDATION_SIZE // global_batch_size)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-81a4e7ffa183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not enough GPU hardware devices available\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Not enough GPU hardware devices available"
          ]
        }
      ]
    }
  ]
}